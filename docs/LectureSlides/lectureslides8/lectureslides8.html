<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STAT 302, Week 8</title>
    <meta charset="utf-8" />
    <meta name="author" content="Bryan Martin" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, title-slide

# STAT 302, Week 8
## Statistical Prediction
### Bryan Martin
### Week of Feb 24, 2020

---





# Outline

1. Training and Testing
2. Cross-validation
3. Statistical Prediction

.middler[**Goal:** Learn the concepts, terminology, and techniques in statistical prediction!]

---
# Acknowledgement

This lecture borrows heavily from the Ryan Tibshirani's [Statistical Prediction lecture](http://www.stat.cmu.edu/~ryantibs/statcomp/lectures/prediction.html) for Statistical Computing at Carnegie Mellon University.

---
class: inverse

.sectionhead[Part 1. Training and Testing]

---


# Recall: Regression

Let's assume we have some data `\(X_1,\ldots,X_p, Y\)` where `\(X_1,\ldots,X_p\)` are `\(p\)` **independent variables/explanatory variables/covariates/predictors** and `\(Y\)` is the **dependent variables/response/outcome**.
We want to know the relationship between our covariates and our response, we can do this with a method called **regression**.
Regression provides us with a statistical method to conduct

* **inference:** assess the relationship between our variables, our statistical model as a whole, predictor importance
  * What is the relationship between sleep and GPA?
  * Is parents' education or parents' income more important for explaining income?
* **prediction:** predict new/future outcomes from new/future covariates
  * Can we predict test scores based on hours spent studying?
  

  
---

# Recall: Linear Regression

Given our response `\(Y\)` and our predictors `\(X_1, \ldots, X_p\)`, a **linear regression model** takes the form:

\\[
\LARGE
`\begin{eqnarray}
&amp;Y &amp;= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon,\\
&amp;\epsilon &amp;\sim N(0,\sigma^2) 
\end{eqnarray}`
\\]


Previously, we focused on using this model for inference (e.g. `\(H_0: \beta_1 = 0\)`). Today, we are shifting our focus to prediction!

---

# Statistical Prediction

Often, we use a linear model even when we know that the "truth" is not a purely linear relationship with normally-distributed errors.

--

Does this mean our model is wrong? Yes, in some sense.


--


Does this mean our model is bad? No, not necessarily!

If we consider a linear model only to be a rough approximation, we can still make use of it for **statistical prediction**!


---
layout: true
# Test Error
---


.middler-nocent[
**Training data:** data we use to train/fit our model (everything we've seen so far)

**Test data:** data we use to evaluate/test the performance of our model 
]

---

Suppose we have training data `\(X_{i1}, \ \ldots,\ X_{ip}, Y_i, \ i = 1,\ldots,n\)` which we use to estimate regression coefficents `\(\hat\beta_0, \ \hat\beta_1, \ldots \hat\beta_p\)` (Recall: we use `\(\hat{}\)` "hats" to indicate **estimates**)

Now suppose we are given new testing data `\(X^*_1, \ \ldots, X^*_{p}\)` and asked to predict the associated `\(Y^âˆ—\)`. 
Using our estimated linear model, the prediction is: 
`$$\hat{Y} = \hat\beta_0 + \hat{\beta}_1 X^*_1 + \cdots + \hat{\beta}_p X^*_p$$`
---

Given our prediction 

`$$\hat{Y} = \hat\beta_0 + \hat{\beta}_1 X^*_1 + \cdots + \hat{\beta}_p X^*_p$$`
We define the **test error**, or **prediction error**, as
`$$\mathbb{E}[(Y^* -\hat{Y}^*)^2].$$`

In other words, the test error is defined as the expected squared difference between a new prediction and the truth!&lt;sup&gt;1&lt;/sup&gt;

.footnote[[1] where the expectation is taken over all random training and test data.]

---

**Test Error:** `\(\mathbb{E}[(Y^* -\hat{Y}^*)^2],\)` the expected squared difference between a new prediction and the truth.

The test error would make a great way to assess the predictive power of our model!

However, how do we estimate it? We don't know the value of that expectation, so somehow we have to use our data to estimate it in order to use it to assess our model. 
This provides us with a tool for

* **predictive assessment:** an understanding of the magnitude of errors we should expect when making future predictions
* **model/method selection:** choose among multiple choices of model based on minimizing test error

---
layout: false
layout: true
# Training Error

---

A natural estimator for the test error might be the observed average **training error:**

`$$\dfrac{1}{n}\sum_{i=1}^n (Y_i - \hat{Y}_i)^2.$$`

Can you think of a problem with this approach?

---

`$$\dfrac{1}{n}\sum_{i=1}^n (Y_i - \hat{Y}_i)^2.$$`
Can you think of a problem with this approach?

In general, this will be *too optimistic*. Why? We chose estimates `\(\hat{\beta}_0,\ \ldots,\ \hat{\beta}_p\)` based on our training data! 
More specifically (with a linear regression model), we chose estimates *such that* they minimize the error from our training data.
We cannot expect this error rate to generalize to test data.

Critically, the more complex our model, the more optimistic our training data will be as an estimate for our test data!

---
layout: false
layout: true

# Training Error vs Test Error Examples
---


```r
library(ggpubr)
# generate data
set.seed(302)
n &lt;- 30
x &lt;- sort(runif(n, -3, 3))
y &lt;- 2*x + 2*rnorm(n)
x_test &lt;- sort(runif(n, -3, 3))
y_test &lt;- 2*x_test + 2*rnorm(n)
df_train &lt;- data.frame("x" = x, "y" = y)
df_test &lt;- data.frame("x" = x_test, "y" = y_test)

# store a theme
my_theme &lt;- theme_bw(base_size = 16) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5))

# generate plots
g_train &lt;- ggplot(df_train, aes(x = x, y = y)) + geom_point() +
  xlim(-3, 3) + ylim(min(y, y_test), max(y, y_test)) + 
  labs(title = "Training Data") + my_theme
g_test &lt;- ggplot(df_test, aes(x = x, y = y)) + geom_point() +
  xlim(-3, 3) + ylim(min(y, y_test), max(y, y_test)) + 
  labs(title = "Test Data") + my_theme
ggarrange(g_train, g_test) # from ggpubr, to put side-by-side
```


---

.middler[
&lt;img src="lectureslides8_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;
]

---


```r
# Fit linear model and calculate training error
lm_fit &lt;- lm(y ~ x, data = df_train)
yhat_train &lt;- predict(lm_fit)
# (y_i - yhat_i)^2
train_err &lt;- mean((df_train$y - yhat_train)^2)

# Calculate testing error
yhat_test &lt;- predict(lm_fit, data.frame(x = df_test$x))
test_err &lt;- mean((df_test$y - yhat_test)^2)

# Add linear model and error text to plot
g_train2 &lt;- g_train + 
  labs(subtitle = paste("Training error:", round(train_err, 3))) +
  geom_line(aes(y = fitted(lm_fit)), col = "red", lwd = 1.5)
g_test2 &lt;- g_test + 
  labs(subtitle = paste("Test error:", round(test_err, 3))) +
  geom_line(aes(y = fitted(lm_fit), x = df_train$x), col = "red", lwd = 1.5)

ggarrange(g_train2, g_test2)
```


---

## Degree 1 Model

.middler[
&lt;img src="lectureslides8_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;
]

---


```r
# Fit 5 degree polynomial and calculate training error
lm_fit_5 &lt;- lm(y ~ poly(x, 5), data = df_train)
yhat_train_5 &lt;- predict(lm_fit_5)
train_err_5 &lt;- mean((df_train$y - yhat_train_5)^2)

# Calculate testing error
yhat_test_5 &lt;- predict(lm_fit_5, data.frame(x = df_test$x))
test_err_5 &lt;- mean((df_test$y - yhat_test_5)^2)

# Create smooth line for plotting
x_fit &lt;- data.frame(x = seq(-3, 3, length = 100))
line_fit &lt;- data.frame(x = x_fit, y = predict(lm_fit_5, newdata = x_fit))
                       
# Add linear model and error text to plot
g_train3 &lt;- g_train + 
  labs(subtitle = paste("Training error:", round(train_err_5, 3))) +
  geom_line(data = line_fit, aes(y = y, x = x), col = "red", lwd = 1.5)
g_test3 &lt;- g_test + 
  labs(subtitle = paste("Test error:", round(test_err_5, 3))) +
  geom_line(data = line_fit, aes(y = y, x = x), col = "red", lwd = 1.5)

ggarrange(g_train3, g_test3)
```

---

## Degree 5 Model

.middler[
&lt;img src="lectureslides8_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;
]

---


```r
# Fit 10 degree polynomial and calculate training error
lm_fit_10 &lt;- lm(y ~ poly(x, 10), data = df_train)
yhat_train_10 &lt;- predict(lm_fit_10)
train_err_10 &lt;- mean((df_train$y - yhat_train_10)^2)

# Calculate testing error
yhat_test_10 &lt;- predict(lm_fit_10, data.frame(x = df_test$x))
test_err_10 &lt;- mean((df_test$y - yhat_test_10)^2)

# Create smooth line for plotting
x_fit &lt;- data.frame(x = seq(-3, 3, length = 100))
line_fit &lt;- data.frame(x = x_fit, y = predict(lm_fit_10, newdata = x_fit))
                       
# Add linear model and error text to plot
g_train4 &lt;- g_train + 
  labs(subtitle = paste("Training error:", round(train_err_10, 3))) +
  geom_line(data = line_fit, aes(y = y, x = x), col = "red", lwd = 1.5)
g_test4 &lt;- g_test + 
  labs(subtitle = paste("Test error:", round(test_err_10, 3))) +
  geom_line(data = line_fit, aes(y = y, x = x), col = "red", lwd = 1.5)

ggarrange(g_train4, g_test4)
```

---

## Degree 10 Model

.middler[
&lt;img src="lectureslides8_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
]

---


```r
# Fit 15 degree polynomial and calculate training error
lm_fit_15 &lt;- lm(y ~ poly(x, 15), data = df_train)
yhat_train_15 &lt;- predict(lm_fit_15)
train_err_15 &lt;- mean((df_train$y - yhat_train_15)^2)

# Calculate testing error
yhat_test_15 &lt;- predict(lm_fit_15, data.frame(x = df_test$x))
test_err_15 &lt;- mean((df_test$y - yhat_test_15)^2)

# Create smooth line for plotting
x_fit &lt;- data.frame(x = seq(-3, 3, length = 100))
line_fit &lt;- data.frame(x = x_fit, y = predict(lm_fit_15, newdata = x_fit))
                       
# Add linear model and error text to plot
g_train5 &lt;- g_train + 
  labs(subtitle = paste("Training error:", round(train_err_15, 3))) +
  geom_line(data = line_fit, aes(y = y, x = x), col = "red", lwd = 1.5)
g_test5 &lt;- g_test + 
  labs(subtitle = paste("Test error:", round(test_err_15, 3))) +
  geom_line(data = line_fit, aes(y = y, x = x), col = "red", lwd = 1.5)

ggarrange(g_train5, g_test5)
```

---

## Degree 15 Model

.middler[
&lt;img src="lectureslides8_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;
]

---
layout: false
layout: true

# Bias-Variance Tradeoff

---

.middler-nocent[
The formula for test error can be decomposed as a function of the bias and variance of our estimates.

* **Bias:** the expected difference between our estimate and the truth

* **Variance:** the variability of our estimate of the truth
]

---

.center[&lt;img src="bvtradeoff.png" alt="" height="450"/&gt;]

Image Credit: An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani

---

.middler-nocent[
* **Bias:** underfitting, error from missing relevant relationships

* **Variance:** overfitting, error from high sensitivity
]

---

.middler[&lt;img src="underover1.png" alt="" height="300"/&gt;]

.footnote[[Image Source](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)]


---

.middler[&lt;img src="underover2.png" alt="" height="300"/&gt;]

.footnote[[Image Source](https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76)]

---

It is easy to fit a model with extremely low variance and extremely high bias, or extremely low bias and extremely high variance, but these models will not have a good test error.
Ideally, we would like an estimate that has both low bias and low variance, but this is not always possible!

As a general rule of thumb, as we increase the complexity of our model, the bias will increase but the variance will decrease.
You can think of our bias increasing because we are training our model to be more specific to our training data.
You can think of the variance decreasing because our model will be more precisely defined.


---
layout: false
class: inverse

.sectionhead[Part 2. Cross-validation]


---
layout: true

# Sample-splitting
---

Where do we get our "training data" and our "test data" in practice? We can't just simulate more data for our test data in the real world. 
In practice, we use a technique called **sample-splitting:**

1. Split the data set into two (or more...) parts
2. First part of data: train the model/method
3. Second part of data: make predictions
4. Evaluate observed test error

---


```r
# Generate data
set.seed(302)
n &lt;- 50
x &lt;- sort(runif(n, -3, 3))
y &lt;- 2*x + 2*rnorm(n)

# Randomly split data into two parts
inds &lt;- sample(rep(1:2, length = n))
split &lt;- as.factor(inds) %&gt;%
  fct_recode(Training = "1", Test = "2")
data &lt;- data.frame("x" = x, "y" = y, "split" = split)

g_split &lt;- ggplot(data, aes(x = x, y = y, color = split)) + geom_point() +
  labs(title = "Sample-splitting") + my_theme
g_split
```


---

&lt;img src="lectureslides8_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;
---


```r
# Train on the training split
data_train &lt;- data %&gt;% filter(split == "Training")
lm_1 &lt;- lm(y ~ x, data = data_train)
lm_10 &lt;- lm(y ~ poly(x, 10), data = data_train)

# Predict on the second half
data_test &lt;- data %&gt;% filter(split == "Test")
pred_1 &lt;- predict(lm_1, data.frame(x = data_test$x))
pred_10 &lt;- predict(lm_10, data.frame(x = data_test$x))

# Calculate test error
test_err_1 &lt;- mean((data_test$y - pred_1)^2)
test_err_10 &lt;- mean((data_test$y - pred_10)^2)
```


---


```r
# Create smooth lines for plotting
x_fit &lt;- data.frame(x = seq(min(data$x), max(data$x), length = 100))
line_fit_1 &lt;- data.frame(x = x_fit, y = predict(lm_1, newdata = x_fit))
line_fit_10 &lt;- data.frame(x = x_fit, y = predict(lm_10, newdata = x_fit))
                       
# Add linear model and error text to plot
g_split1 &lt;- g_split + 
  labs(subtitle = paste("Degree 1 Test error:", round(test_err_1, 3))) +
  geom_line(data = line_fit_1, aes(y = y, x = x), col = "black", lwd = 1.5)
g_split10 &lt;- g_split + 
  labs(subtitle = paste("Degree 10 Test error:", round(test_err_10, 3))) +
  geom_line(data = line_fit_10, aes(y = y, x = x), col = "black", lwd = 1.5)

ggarrange(g_split1, g_split10, common.legend = TRUE, legend = "bottom")
```

---

&lt;img src="lectureslides8_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;


---


Sample-splitting gives us a powerful tool to evaluate the performance of estimator *on data that was not used to train it*! Of course, this comes at the cost of sacrificing some of our data that could be used in order to train a model with as much information as possible, but this is a sacrifice we must make in practice if we want to appropriately evaluate the predictive performance of our models/methods!

However, we can usually do better than just splitting our data in half.

---
layout: false
layout: true

# Cross-validation
---

Wednesday!
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "tomorrow-night-bright",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
